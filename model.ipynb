{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hmuj2\\miniconda3\\envs\\ElectrifyEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResistorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, annotation_file, transforms=None):\n",
    "        self.img_dir = img_dir  # Add this line\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.transforms = transforms\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "        # Filter out images without annotations\n",
    "        self.ids = [img_id for img_id in self.ids if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n",
    "\n",
    "    def get_boxes(self, anns):\n",
    "        boxes = []\n",
    "        for ann in anns:\n",
    "            x, y, width, height = ann['bbox']\n",
    "            boxes.append([x, y, x + width, y + height])\n",
    "        return torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the image ID and load the image\n",
    "        img_id = self.ids[index]\n",
    "        img = self.coco.loadImgs(img_id)[0]\n",
    "        path = os.path.join(self.img_dir, img['file_name'])\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Get the annotations for the image\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # Create masks for each annotation\n",
    "        masks = []\n",
    "        for ann in anns:\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "\n",
    "        if len(masks) > 1:\n",
    "            masks = np.stack(masks, axis=0)\n",
    "        else:\n",
    "            masks = np.array(masks)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms is not None:\n",
    "            image, masks = self.transforms(image, masks)\n",
    "\n",
    "        # Convert masks to target format\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        # Create target dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = self.get_boxes(anns)\n",
    "        target[\"labels\"] = torch.ones((len(anns),), dtype=torch.int64)\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = torch.tensor([img_id], dtype=torch.int64)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    def transform(img, masks):\n",
    "        img_transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        img = img_transform(img)\n",
    "        masks = np.stack(masks, axis=0)\n",
    "        return img, masks\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ResistorDataset(\"ElectrifyReloaded.v2-model1.coco-segmentation/train\", \"ElectrifyReloaded.v2-model1.coco-segmentation/train/_annotations.coco.json\", get_transforms())\n",
    "val_dataset = ResistorDataset(\"ElectrifyReloaded.v2-model1.coco-segmentation/valid\", \"ElectrifyReloaded.v2-model1.coco-segmentation/valid/_annotations.coco.json\", get_transforms())\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # Load pre-trained Mask R-CNN model\n",
    "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Replace the box and mask predictors with new ones\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, device, num_epochs=10):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print('-' * 10)\n",
    "        running_loss = 0.0\n",
    "        num_iter = 0\n",
    "        \n",
    "        for images, targets in dataloaders['train']:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            total_loss = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += total_loss.item()\n",
    "            num_iter += 1\n",
    "        \n",
    "        average_loss = running_loss / num_iter\n",
    "        print(f\"Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a dictionary containing both DataLoaders\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n",
    "# Instantiate the model\n",
    "model = get_model_instance_segmentation(num_classes=2)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10  # Adjust this value as needed\n",
    "trained_model = train_model(model, dataloaders, device, num_epochs=num_epochs)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_model.state_dict(), \"trained_resistor_detection_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a dictionary containing both DataLoaders\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n",
    "model = get_model_instance_segmentation(num_classes=2)\n",
    "model.load_state_dict(torch.load(\"trained_resistor_detection_model.pt\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_polygons(output, threshold=0.5):\n",
    "    masks = output['masks'].cpu().detach().numpy()\n",
    "    polygons = []\n",
    "    for mask in masks:\n",
    "        mask = mask[0]\n",
    "        mask = (mask > threshold).astype(np.uint8)\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for contour in contours:\n",
    "            contour = contour.flatten().tolist()\n",
    "            if len(contour) > 4:\n",
    "                polygons.append(contour)\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "input_image = Image.open(\"3_Resistors.jpg\").convert(\"RGB\")\n",
    "input_tensor = ToTensor()(input_image)\n",
    "input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_polygons(image, polygons):\n",
    "    image_copy = image.copy()\n",
    "    for polygon in polygons:\n",
    "        pts = np.array(polygon, np.int32).reshape((-1, 1, 2))\n",
    "        cv2.polylines(image_copy, [pts], True, (0, 255, 0), 2)\n",
    "    return image_copy\n",
    "\n",
    "polygons = get_polygons(output[0])\n",
    "result_image = draw_polygons(np.array(input_image), polygons)\n",
    "\n",
    "plt.imshow(result_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
